# llm-math-finetuning
Math Problem Generator via LLM Fine-tuning

This project demonstrates fine-tuning of the OLMo-1B model to generate elementary math problems.
The implementation uses LoRA (Low-Rank Adaptation) for efficient fine-tuning of large language models.

Key components:
- Model baseline evaluation
- Finetuning dataset selection and exploration
- LoRA-based fine-tuning implementation
- Inference with the fine-tuned model
- Performance evaluation and analysis

This project is heavily inspired by one of the assignments in the PracticalGenAI course from Nebius Academy [link](https://academy.nebius.com/generative-ai/)
